/**
 * (C) Copyright 2020, 2021, 2022, 2023, 2024 IBM. All Rights Reserved.
 *
 * Licensed under the MIT license. See LICENSE file in the project root for details.
 */

#ifdef RPU_USE_CUDA
#define __RPU_CUDA_HALF_DEFINED
#include "cuda.h"
#include "cuda_util.h"
#include "rpu_base.h"

#include "rpucuda.h"
#include "rpucuda_pulsed.h"
#include "rpucuda_lrtt_transfer_device.h"
#include "rpucuda_vector_device.h"
#include <ATen/cuda/CUDAContext.h>

#define CHECK_CUDA(x)                                                                              \
  TORCH_CHECK(                                                                                     \
      x.device().type() == torch::kCUDA, #x " must be a CUDA tensor. got ", x.device().type(),     \
      " versus ", torch::kCUDA)
#define CHECK_CUDA_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
#define CHECK_CUDA_DTYPE(x)                                                                        \
  TORCH_CHECK(x.dtype() == DEFAULT_DTYPE, #x " must be of the right data type.")
#define CHECK_TORCH_CUDA_INPUT(x)                                                                  \
  CHECK_CUDA(x);                                                                                   \
  CHECK_CUDA_CONTIGUOUS(x);                                                                        \
  CHECK_CUDA_DTYPE(x)

#define NAME(S) (S + type_name_add).c_str()

template <typename T, typename T_RPU>
void declare_rpu_tiles_cuda(py::module &m, std::string type_name_add, bool add_utilities) {
  using Class = RPU::RPUCudaSimple<T_RPU>;
  using ClassPulsed = RPU::RPUCudaPulsed<T_RPU>;

  /*
   * RPU definitions.
   */

  if (add_utilities) {
    // Helper bindings.
    py::class_<cudaStream_t>(m, "cudaStream_t");
  }

  py::class_<Class, RPU::RPUSimple<T_RPU>>(
      m, NAME("CudaFloatingPointTile"),
      R"pbdoc(
    Floating point tile (CUDA).

    Args:
        tile: existing ``FloatingPointTile`` that will be copied.
    )pbdoc")
      .def(
          py::init([](RPU::RPUSimple<T_RPU> &rpu) {
            auto ret = std::unique_ptr<Class>(new Class(at::cuda::getCurrentCUDAStream(), rpu));
            return ret;
          }),
          py::arg("cpu_tile"))
      .def("__copy__", [](const Class &self) { return Class(self); })
      .def(
          "__deepcopy__", [](const Class &self, py::dict) { return Class(self); }, py::arg("memo"))
      .def(
          "dump_extra",
          [](Class &self) {
            RPU::state_t state;
            self.dumpExtra(state, "rpucuda");
            return state;
          },
          R"pbdoc(
           Return additional state variables for pickling. 
 
           Returns:
               state: dictionary of extra variables states
           )pbdoc")
      .def(
          "load_extra",
          [](Class &self, RPU::state_t state, bool strict) {
            self.loadExtra(state, "rpucuda", strict);
          },
          py::arg("state"), py::arg("strict"),
          R"pbdoc(
           Load the state dictionary generated by dump_extra.

           Args:
               strict: Whether to throw a runtime error when a field is not found. 
           )pbdoc")
      .def(
          "set_shared_weights",
          [](Class &self, torch::Tensor weights) {
            CHECK_TORCH_CUDA_INPUT(weights);
            // weight is d major for CUDA !
            if (weights.dim() != 2 || weights.size(1) != self.getDSize() ||
                weights.size(0) != self.getXSize()) {
              throw std::runtime_error(
                  "Invalid weights dimensions: expected [" + std::to_string(self.getXSize()) + "," +
                  std::to_string(self.getDSize()) + "] array");
            }
            if (weights.device().index() != self.getGPUId()) {
              throw std::runtime_error(
                  "Weights need to be on the same cuda device: expected " +
                  std::to_string(self.getGPUId()) + ", got " +
                  std::to_string(weights.device().index()));
            }

            std::lock_guard<std::mutex> lock(self.mutex_);
            return self.setSharedWeights(reinterpret_cast<T_RPU *>(weights.data_ptr<T>()));
          },
          py::arg("weights"))
      .def(
          "set_delta_weights",
          [](Class &self, torch::Tensor delta_weights) {
            CHECK_TORCH_CUDA_INPUT(delta_weights);
            // weight is d major for CUDA !
            if (delta_weights.dim() != 2 || delta_weights.size(1) != self.getDSize() ||
                delta_weights.size(0) != self.getXSize()) {
              throw std::runtime_error(
                  "Invalid delta weights dimensions: expected [" + std::to_string(self.getXSize()) +
                  "," + std::to_string(self.getDSize()) + "] array");
            }
            if (delta_weights.device().index() != self.getGPUId()) {
              throw std::runtime_error(
                  "Delta weights need to be on the same cuda device: expected " +
                  std::to_string(self.getGPUId()) + ", got " +
                  std::to_string(delta_weights.device().index()));
            }

            std::lock_guard<std::mutex> lock(self.mutex_);
            return self.setDeltaWeights(reinterpret_cast<T_RPU *>(delta_weights.data_ptr<T>()));
          },
          py::arg("delta_weights"))
      .def(
          "remap_weights",
          [](Class &self, ::RPU::WeightRemapParameter &wrmpar, torch::Tensor scales) {
            CHECK_TORCH_CUDA_INPUT(scales);
            if ((scales.numel() != self.getDSize()) || scales.dim() != 1) {
              throw std::runtime_error(
                  "Invalid scales dimensions: expected [" + std::to_string(self.getDSize()) +
                  "] tensor");
            }
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.remapWeights(wrmpar, reinterpret_cast<T_RPU *>(scales.data_ptr<T>()));
            return scales;
          },
          py::arg("weight_remap_params"), py::arg("scales"),
          R"pbdoc(
           Remaps the weights for use of hardware-aware training.

           Several remap types are available, see ``WeightRemapParameter``.

           Args:
               weight_remap_params: parameters of the remapping.
               scales: scales that will be used and updated during remapping

           Returns:
               torch::tensor: ``[d_size]`` of scales

           )pbdoc")

      .def(
          "forward",
          [](Class &self, const torch::Tensor &x_input_, bool bias = false, bool x_trans = false,
             bool d_trans = false, bool is_test = false, bool non_blocking = false) {
            auto x_input = x_input_.contiguous();
            CHECK_TORCH_CUDA_INPUT(x_input);
            if (x_input.device().index() != self.getGPUId())
              throw std::runtime_error("x_input must be on CUDA device " + std::to_string(self.getGPUId()));

            if (x_input.dim() < 1) {
              throw std::runtime_error(
                  "Invalid x_input dimensions: expected at least 1 dimensional array");
            }
            int in_size = x_trans ? x_input.size(0) : x_input.size(-1);
            int expected_in_size = self.getXSize() - (bias ? 1 : 0);
            int m_batch = x_input.numel() / in_size;
            int out_size = self.getDSize();
            // Validate the x_input dimensions.
            if (in_size != expected_in_size) {
              std::string shape_str = x_trans ? ("[*, " + std::to_string(expected_in_size) + "]")
                                              : ("[" + std::to_string(expected_in_size) + ",*]");
              throw std::runtime_error(
                  "Invalid x_input dimensions: expected " + shape_str + " array");
            }

            // Build the buffers.
            std::vector<int64_t> dims(x_input.sizes().begin(), x_input.sizes().end());
            if (d_trans) {
              dims[0] = out_size;
            } else {
              dims[dims.size() - 1] = out_size;
            }
            torch::Tensor d_output = torch::empty(dims, x_input.options());

            // Call RPU function.
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            self.forward(
                reinterpret_cast<T_RPU *>(x_input.template data_ptr<T>()),
                reinterpret_cast<T_RPU *>(d_output.template data_ptr<T>()), bias, m_batch, x_trans,
                d_trans, is_test);

            if (!non_blocking) {
              self.finishAllCalculations();
            }
            self.releaseExternalStream();
            return d_output;
          },
          py::arg("x_input"), py::arg("bias") = false, py::arg("x_trans") = false,
          py::arg("d_trans") = false, py::arg("is_test") = false, py::arg("non_blocking") = false,
          R"pbdoc(
           Compute the dot product (forward pass).

           Compute the dot product:
           .. math:

               \mathbf{y} = W\mathbf{x} [+ \mathbf{b}]

           where :math:`\mathbf{x}` is the input and :math:`W` is the ``[d_size, x_size]``
           current weight matrix. If ``bias`` is True, then it is assumes that a
           bias row is added to the analog tile weights.  The input :math:`\mathbf{x}` is
           then  expected to be of size ``x_size -1`` , as internally it will be
           expanded by a 1, to match the bias row in the tile weights.

           An analog tile will have a possible non-ideal version of this forward pass.

           Args:
               x_input: ``[N,*, x_size (- 1)]`` input :math:`\mathbf{x}` torch::Tensor.
               bias: whether to use bias.
               x_trans: whether the ``x_input`` matrix is transposed. That is of size ``[x_size (- 1), *, N]``
               d_trans: whether the ``d`` matrix is transposed.
               is_test: whether inference (true) mode or training (false)
               non_blocking: whether to not sync the cuda execution

           Returns:
               torch::tensor: ``[N, *, d_size]`` or ``[d_size, *, N]`` matrix.
           )pbdoc")

      .def(
          "backward",
          [](Class &self, const torch::Tensor &d_input_, bool bias = false, bool d_trans = false,
             bool x_trans = false, bool non_blocking = false) {
            auto d_input = d_input_.contiguous();
            CHECK_TORCH_CUDA_INPUT(d_input);
            if (d_input.device().index() != self.getGPUId())
              throw std::runtime_error("d_input must be on CUDA device " + std::to_string(self.getGPUId()));

            if (d_input.dim() < 1) {
              throw std::runtime_error(
                  "Invalid d_input dimensions: expected at least 1 dimensional array");
            }
            int in_size = d_trans ? d_input.size(0) : d_input.size(-1);
            int expected_in_size = self.getDSize();
            int m_batch = d_input.numel() / in_size;
            int out_size = self.getXSize() - (bias ? 1 : 0);

            // Validate the d_input dimensions.
            if (in_size != expected_in_size) {
              std::string shape_str = d_trans ? ("[*, " + std::to_string(expected_in_size) + "]")
                                              : ("[" + std::to_string(expected_in_size) + ", *]");
              throw std::runtime_error(
                  "Invalid d_input dimensions: expected " + shape_str + " array");
            }

            // Build the buffers.
            std::vector<int64_t> dims(d_input.sizes().begin(), d_input.sizes().end());
            if (x_trans) {
              dims[0] = out_size;
            } else {
              dims[dims.size() - 1] = out_size;
            }
            torch::Tensor x_output = torch::empty(dims, d_input.options());

            // Call RPU function.
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            self.backward(
                reinterpret_cast<T_RPU *>(d_input.template data_ptr<T>()),
                reinterpret_cast<T_RPU *>(x_output.template data_ptr<T>()), bias, m_batch, d_trans,
                x_trans);

            if (!non_blocking) {
              self.finishAllCalculations();
            }
            self.releaseExternalStream();
            return x_output;
          },
          py::arg("d_input"), py::arg("bias") = false, py::arg("d_trans") = false,
          py::arg("x_trans") = false, py::arg("non_blocking") = false,
          R"pbdoc(
           Compute the transposed dot product (backward pass).

           Compute the transposed dot product:
           .. math:

               \mathbf{y} = W\mathbf{d}

           where :math:`\mathbf{d}` is the input and :math:`W` is the  current
           weight matrix (of size ``[d_size, x_size]``).

           An analog tile will have a possible non-ideal version of this backward pass.

           Args:
               d_input: ``[N, *,  d_size]`` input :math:`\mathbf{d}` torch::Tensor.
               bias: whether to use bias.
               d_trans: whether the ``d_input`` matrix is transposed. That is of size ``[d_size, *, N]``
               x_trans: whether the ``x`` output matrix is transposed.
               non_blocking: whether to not sync the cuda execution

           Returns:
               torch::Tensor: ``[N, *, x_size (-1)]`` or ``[x_size (-1), *, N]`` torch::Tensor.
           )pbdoc")

      .def(
          "update",
          [](Class &self, const torch::Tensor &x_input_, const torch::Tensor &d_input_,
             bool bias = false, bool x_trans = false, bool d_trans = false,
             bool non_blocking = false) {
            T_RPU lr = self.getLearningRate();
            if (lr == (T_RPU)0.0) {
              return;
            }

            auto d_input = d_input_.contiguous();
            auto x_input = x_input_.contiguous();

            CHECK_TORCH_CUDA_INPUT(d_input);
            CHECK_TORCH_CUDA_INPUT(x_input);
            if (x_input.device().index() != self.getGPUId() ||
                d_input.device().index() != self.getGPUId())
              throw std::runtime_error("x_input/d_input must be on CUDA device " + std::to_string(self.getGPUId()));

            if ((x_input.dim() < 1) || (d_input.dim() < 1)) {
              throw std::runtime_error(
                  "Invalid x_input/d_input dimensions: expected at least 1 dimensional array");
            }

            int in_size = x_trans ? x_input.size(0) : x_input.size(-1);
            int expected_in_size = self.getXSize() - (bias ? 1 : 0);
            int m_batch = x_input.numel() / in_size;

            int out_size = d_trans ? d_input.size(0) : d_input.size(-1);
            int expected_out_size = self.getDSize();
            int m_batch_from_d = d_input.numel() / out_size;

            // Validate the x_input dimensions.
            if (in_size != expected_in_size) {
              std::string shape_str = x_trans ? ("[*, " + std::to_string(expected_in_size) + "]")
                                              : ("[" + std::to_string(expected_in_size) + ", *]");
              throw std::runtime_error(
                  "Invalid x_input dimensions: expected " + shape_str + " array");
            }
            // Validate the d_input dimensions.
            if (out_size != expected_out_size) {
              std::string shape_str = d_trans ? ("[*, " + std::to_string(expected_out_size) + "]")
                                              : ("[" + std::to_string(expected_out_size) + ", *]");
              throw std::runtime_error(
                  "Invalid d_input dimensions: expected " + shape_str + " array");
            }

            if (m_batch != m_batch_from_d) {
              throw std::runtime_error(
                  "Invalid x_input or d_input dimensions: batch dimensions mismatch!");
            }

            // Call RPU function.
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            self.update(
                reinterpret_cast<T_RPU *>(x_input.template data_ptr<T>()),
                reinterpret_cast<T_RPU *>(d_input.template data_ptr<T>()), bias, m_batch, x_trans,
                d_trans);

            if (!non_blocking) {
              self.finishAllCalculations();
            }
            self.releaseExternalStream();
          },
          py::arg("x_input"), py::arg("d_input"), py::arg("bias") = false, 
          py::arg("x_trans") = false, py::arg("d_trans") = false, py::arg("non_blocking") = false,
          R"pbdoc(
           Compute an n-rank update.

           Compute an n-rank update:
           .. math:

               W \leftarrow W - \lambda \mathbf{x}\mathbf{d}^T

           where :math:`\lambda` is the learning rate.

           An analog tile will have a possible non-ideal version of this update pass.

           Note:
               The learning rate is always positive, and thus scaling is negative.

           Args:
               x_input: ``[N, *, x_size (-1)]`` input :math:`\mathbf{x}` torch::Tensor.
               d_input: ``[N, *, d_size]`` input :math:`\mathbf{d}` torch::Tensor.
               bias: whether to use bias.
               x_trans: whether the ``x_input`` matrix is transposed, ie. ``[x_size (-1), *, N]``
               d_trans: whether the ``d`` matrix is transposed, ie. ``[d_size, *, N]``
               non_blocking: Whether to not sync the cuda execution
           )pbdoc")
      .def(
          "forward_indexed",
          [](Class &self, const torch::Tensor &x_input_, const torch::Tensor &d_tensor_,
             bool is_test = false, bool non_blocking = false) {
            auto x_input = x_input_.contiguous();
            auto d_tensor = d_tensor_.contiguous();
            CHECK_TORCH_CUDA_INPUT(x_input);
            CHECK_TORCH_CUDA_INPUT(d_tensor);

            int N = x_input.size(0); // batch
            int d_image_size = ((d_tensor.numel() / d_tensor.size(0)) / d_tensor.size(1));

            // Call RPU function.
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            self.forwardIndexed(
                reinterpret_cast<T_RPU *>(x_input.template data_ptr<T>()),
                reinterpret_cast<T_RPU *>(d_tensor.template data_ptr<T>()), x_input.numel(),
                d_image_size, N, true, is_test);

            if (!non_blocking) {
              self.finishAllCalculations();
            }
            self.releaseExternalStream();
            return d_tensor;
          },
          py::arg("x_input"), py::arg("d_tensor"), py::arg("is_test") = false,
          py::arg("non_blocking") = false,
          R"pbdoc(
           Compute the dot product using an index matrix (forward pass).

           Caution:
               Internal use for convolutions only.

           Args:
               x_input: 4D torch::tensor in order N,C,H,W
               d_tensor: torch:tensor with convolution dimensions
               is_test: whether inference (true) mode or training (false)
               non_blocking: Whether to not sync the cuda execution

           Returns:
               d_output: 4D torch::tensor in order N,C,d_height,d_width
           )pbdoc")
      .def(
          "backward_indexed",
          [](Class &self, const torch::Tensor &d_input_, const torch::Tensor &x_tensor_,
             bool non_blocking = false) {
            auto d_input = d_input_.contiguous();
            auto x_tensor = x_tensor_.contiguous();
            CHECK_TORCH_CUDA_INPUT(d_input);
            CHECK_TORCH_CUDA_INPUT(x_tensor);

            int N = d_input.size(0); // batch
            int d_image_size = ((d_input.numel() / d_input.size(0)) / d_input.size(1));

            // Call RPU function.
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            self.backwardIndexed(
                reinterpret_cast<T_RPU *>(d_input.template data_ptr<T>()),
                reinterpret_cast<T_RPU *>(x_tensor.template data_ptr<T>()), x_tensor.numel(),
                d_image_size, N, true);

            if (!non_blocking) {
              self.finishAllCalculations();
            }
            self.releaseExternalStream();
            return x_tensor;
          },
          py::arg("d_input"), py::arg("x_tensor"), py::arg("non_blocking") = false,
          R"pbdoc(
           Compute the dot product using an index matrix (backward pass).

           Caution:
              Internal use for convolutions only.

           Args:
              d_input: 4D torch::tensor in order N,C,H,W
              x_tensor: torch:tensor with convolution dimensions
              non_blocking: Whether to not sync the cuda execution

           Returns:
              x_output: 4D torch::tensor in order N,C,x_height,x_width
           )pbdoc")
      .def(
          "update_indexed",
          [](Class &self, const torch::Tensor &x_input_, const torch::Tensor &d_input_,
             bool non_blocking = false) {
            auto x_input = x_input_.contiguous();
            auto d_input = d_input_.contiguous();
            CHECK_TORCH_CUDA_INPUT(x_input);
            CHECK_TORCH_CUDA_INPUT(d_input);

            int N = d_input.size(0); // batch
            int d_image_size = d_input.numel() / (d_input.size(0) * d_input.size(1));

            // Call RPU function.
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            self.updateIndexed(
                reinterpret_cast<T_RPU *>(x_input.template data_ptr<T>()),
                reinterpret_cast<T_RPU *>(d_input.template data_ptr<T>()), x_input.numel(),
                d_image_size, N, true);

            if (!non_blocking) {
              self.finishAllCalculations();
            }
            self.releaseExternalStream();
          },
          py::arg("x_input"), py::arg("d_input"), py::arg("non_blocking") = false,
          R"pbdoc(
           Compute the dot product using an index matrix (backward pass).

           Caution:
               Internal use for convolutions only.

           Args:
               x_input: 4D torch::tensor input in order N,C,H,W
               d_input: 4D torch::tensor (grad_output) in order N,C,oH,oW
               non_blocking: Whether to not sync the cuda execution

           )pbdoc");

  py::class_<ClassPulsed, RPU::RPUCudaSimple<T_RPU>>(
      m, NAME("CudaAnalogTile"),
      R"pbdoc(
    Analog tile (CUDA).

    Args:
        tile: existing ``AnalogTile`` that will be copied.
    )pbdoc")
      .def(
          py::init([](RPU::RPUPulsed<T_RPU> &rpu) {
            // TODO: why does directly passing a stream is a problem?
            return std::unique_ptr<ClassPulsed>(
                new ClassPulsed(at::cuda::getCurrentCUDAStream(), rpu));
          }),
          py::arg("tile"))
      .def("__copy__", [](const ClassPulsed &self) { return ClassPulsed(self); })
      .def(
          "__deepcopy__", [](const ClassPulsed &self, py::dict) { return ClassPulsed(self); },
          py::arg("memo"))
      .def("get_meta_parameters", &ClassPulsed::getMetaPar)
      .def(
          "lrtt_compose_w_eff",
          [](ClassPulsed &self, double alpha) {
            // Direct top-level cast to LR-TT device
            auto& dev = self.getRPUDeviceCuda();
            auto* lrtt_dev = dynamic_cast<RPU::LRTTTransferRPUDeviceCuda<T_RPU>*>(
                const_cast<RPU::AbstractRPUDeviceCuda<T_RPU>*>(&dev));
            if (!lrtt_dev) {
              throw std::runtime_error("lrtt_compose_w_eff requires an LR-TT tile (not found)");
            }

            const int d = self.getDSize();
            const int x = self.getXSize();

            // Column-major output buffer: shape [d, x], stride [1, d]
            auto opts = torch::TensorOptions()
                .dtype(c10::CppTypeToScalarType<T>::value)
                .device(torch::kCUDA, self.getGPUId())
                .requires_grad(false);
            torch::Tensor w_eff = torch::empty_strided({d, x}, {1, d}, opts);

            // Launch on the current PyTorch stream
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            lrtt_dev->composeForwardInject(
                static_cast<T_RPU>(alpha),
                reinterpret_cast<T_RPU*>(w_eff.template data_ptr<T>()),
                self.getStream());
            self.finishAllCalculations();
            self.releaseExternalStream();
            return w_eff;
          },
          py::arg("alpha"),
          R"pbdoc(
           Compose effective weights W_eff = W_visible + alpha * (A_lr @ B_lr).

           Only for LR-TT tiles. The returned CUDA tensor is **column-major**
           with shape ``[d_size, x_size]`` and strides ``[1, d_size]``.
           
           Note: The effective scale used is alpha_in Ã— lora_alpha from the device meta-parameters.

           Args:
               alpha: LoRA alpha scaling factor

           Returns:
               w_eff: Effective weights [d_size, x_size] in column-major layout
           )pbdoc")
      // LR-TT sub-tile getters
      .def(
          "lrtt_get_visible_weights",
          [](ClassPulsed &self) {
            auto* dev_base = const_cast<RPU::AbstractRPUDeviceCuda<T_RPU>*>(&self.getRPUDeviceCuda());
            auto* lrtt_dev = dynamic_cast<RPU::LRTTTransferRPUDeviceCuda<T_RPU>*>(dev_base);
            if (!lrtt_dev) {
              throw std::runtime_error("lrtt_get_visible_weights requires an LR-TT tile");
            }
            
            const int d = self.getDSize();
            const int x = self.getXSize();
            
            auto opts = torch::TensorOptions()
                .dtype(c10::CppTypeToScalarType<T>::value)
                .device(torch::kCUDA, self.getGPUId())
                .requires_grad(false);
            torch::Tensor weights = torch::empty_strided({d, x}, {1, d}, opts);
            
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            lrtt_dev->copyVisibleWeightsTo(
                reinterpret_cast<T_RPU*>(weights.template data_ptr<T>()),
                self.getStream());
            self.finishAllCalculations();
            self.releaseExternalStream();
            return weights;
          },
          R"pbdoc(
           Get the visible weights sub-tile C from LR-TT device.
           
           Returns:
               weights: Visible weights [d_size, x_size] in column-major layout
           )pbdoc")
      .def(
          "lrtt_get_A_lr",
          [](ClassPulsed &self) {
            auto* dev_base = const_cast<RPU::AbstractRPUDeviceCuda<T_RPU>*>(&self.getRPUDeviceCuda());
            auto* lrtt_dev = dynamic_cast<RPU::LRTTTransferRPUDeviceCuda<T_RPU>*>(dev_base);
            if (!lrtt_dev) {
              throw std::runtime_error("lrtt_get_A_lr requires an LR-TT tile");
            }
            
            const int d = self.getDSize();
            const int rank = lrtt_dev->getRank();
            
            auto opts = torch::TensorOptions()
                .dtype(c10::CppTypeToScalarType<T>::value)
                .device(torch::kCUDA, self.getGPUId())
                .requires_grad(false);
            torch::Tensor A_lr = torch::empty_strided({d, rank}, {1, d}, opts);
            
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            lrtt_dev->copyALRTo(
                reinterpret_cast<T_RPU*>(A_lr.template data_ptr<T>()),
                self.getStream());
            self.finishAllCalculations();
            self.releaseExternalStream();
            return A_lr;
          },
          R"pbdoc(
           Get the A low-rank matrix from LR-TT device.
           
           Returns:
               A_lr: A matrix [d_size, rank] in column-major layout
           )pbdoc")
      .def(
          "lrtt_get_B_lr",
          [](ClassPulsed &self) {
            auto* dev_base = const_cast<RPU::AbstractRPUDeviceCuda<T_RPU>*>(&self.getRPUDeviceCuda());
            auto* lrtt_dev = dynamic_cast<RPU::LRTTTransferRPUDeviceCuda<T_RPU>*>(dev_base);
            if (!lrtt_dev) {
              throw std::runtime_error("lrtt_get_B_lr requires an LR-TT tile");
            }
            
            const int x = self.getXSize();
            const int rank = lrtt_dev->getRank();
            
            auto opts = torch::TensorOptions()
                .dtype(c10::CppTypeToScalarType<T>::value)
                .device(torch::kCUDA, self.getGPUId())
                .requires_grad(false);
            torch::Tensor B_lr = torch::empty_strided({rank, x}, {1, rank}, opts);
            
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            lrtt_dev->copyBLRTo(
                reinterpret_cast<T_RPU*>(B_lr.template data_ptr<T>()),
                self.getStream());
            self.finishAllCalculations();
            self.releaseExternalStream();
            return B_lr;
          },
          R"pbdoc(
           Get the B low-rank matrix from LR-TT device.
           
           Returns:
               B_lr: B matrix [rank, x_size] in column-major layout
           )pbdoc")
      // LR-TT sub-tile setters
      .def(
          "lrtt_set_visible_weights",
          [](ClassPulsed &self, torch::Tensor weights) {
            auto* dev_base = const_cast<RPU::AbstractRPUDeviceCuda<T_RPU>*>(&self.getRPUDeviceCuda());
            auto* lrtt_dev = dynamic_cast<RPU::LRTTTransferRPUDeviceCuda<T_RPU>*>(dev_base);
            if (!lrtt_dev) {
              throw std::runtime_error("lrtt_set_visible_weights requires an LR-TT tile");
            }
            
            const int d = self.getDSize();
            const int x = self.getXSize();
            
            // Shape
            if (weights.dim() != 2 || weights.size(0) != d || weights.size(1) != x) {
              throw std::runtime_error("weights must have shape [d_size, x_size]");
            }
            // Device
            if (!weights.is_cuda() || weights.device().index() != self.getGPUId()) {
              throw std::runtime_error("weights must be a CUDA tensor on the same device as the tile");
            }
            // Dtype
            if (weights.scalar_type() != c10::CppTypeToScalarType<T>::value) {
              throw std::runtime_error("weights dtype must match tile dtype");
            }
            
            // Ensure column-major [d, x], strides [1, d]
            const bool is_col_major = (weights.stride(0) == 1 && weights.stride(1) == d);
            auto opts = weights.options().device(torch::kCUDA, self.getGPUId()).requires_grad(false);
            torch::Tensor cm = is_col_major ? weights
                                            : torch::empty_strided({d, x}, {1, d}, opts);
            if (!is_col_major) {
              cm.copy_(weights);
            }
            
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            lrtt_dev->copyVisibleWeightsFrom(
                reinterpret_cast<const T_RPU*>(cm.template data_ptr<T>()),
                self.getStream());
            self.finishAllCalculations();
            self.releaseExternalStream();
          },
          py::arg("weights"),
          R"pbdoc(
           Set the visible weights sub-tile C in LR-TT device.
           
           Args:
               weights: [d_size, x_size] CUDA tensor on the same device as the tile.
                        Preferred layout: column-major (strides [1, d_size]).
                        Row-major inputs are accepted and internally converted.
           )pbdoc")
      .def(
          "lrtt_set_A_lr",
          [](ClassPulsed &self, torch::Tensor A_lr) {
            auto* dev_base = const_cast<RPU::AbstractRPUDeviceCuda<T_RPU>*>(&self.getRPUDeviceCuda());
            auto* lrtt_dev = dynamic_cast<RPU::LRTTTransferRPUDeviceCuda<T_RPU>*>(dev_base);
            if (!lrtt_dev) {
              throw std::runtime_error("lrtt_set_A_lr requires an LR-TT tile");
            }
            
            const int d = self.getDSize();
            const int rank = lrtt_dev->getRank();
            
            // Shape
            if (A_lr.dim() != 2 || A_lr.size(0) != d || A_lr.size(1) != rank) {
              throw std::runtime_error("A_lr must have shape [d_size, rank]");
            }
            // Device
            if (!A_lr.is_cuda() || A_lr.device().index() != self.getGPUId()) {
              throw std::runtime_error("A_lr must be a CUDA tensor on the same device as the tile");
            }
            // Dtype
            if (A_lr.scalar_type() != c10::CppTypeToScalarType<T>::value) {
              throw std::runtime_error("A_lr dtype must match tile dtype");
            }
            
            // Ensure column-major [d, rank], strides [1, d]
            const bool is_col_major = (A_lr.stride(0) == 1 && A_lr.stride(1) == d);
            auto opts = A_lr.options().device(torch::kCUDA, self.getGPUId()).requires_grad(false);
            torch::Tensor cm = is_col_major ? A_lr
                                            : torch::empty_strided({d, rank}, {1, d}, opts);
            if (!is_col_major) {
              cm.copy_(A_lr);
            }
            
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            lrtt_dev->copyALRFrom(
                reinterpret_cast<const T_RPU*>(cm.template data_ptr<T>()),
                self.getStream());
            self.finishAllCalculations();
            self.releaseExternalStream();
          },
          py::arg("A_lr"),
          R"pbdoc(
           Set the A low-rank matrix in LR-TT device.
           
           Args:
               A_lr: [d_size, rank] CUDA tensor on the same device as the tile.
                     Preferred layout: column-major (strides [1, d_size]).
                     Row-major inputs are accepted and internally converted.
           )pbdoc")
      .def(
          "lrtt_set_B_lr",
          [](ClassPulsed &self, torch::Tensor B_lr) {
            auto* dev_base = const_cast<RPU::AbstractRPUDeviceCuda<T_RPU>*>(&self.getRPUDeviceCuda());
            auto* lrtt_dev = dynamic_cast<RPU::LRTTTransferRPUDeviceCuda<T_RPU>*>(dev_base);
            if (!lrtt_dev) {
              throw std::runtime_error("lrtt_set_B_lr requires an LR-TT tile");
            }
            
            const int x = self.getXSize();
            const int rank = lrtt_dev->getRank();
            
            // Shape
            if (B_lr.dim() != 2 || B_lr.size(0) != rank || B_lr.size(1) != x) {
              throw std::runtime_error("B_lr must have shape [rank, x_size]");
            }
            // Device
            if (!B_lr.is_cuda() || B_lr.device().index() != self.getGPUId()) {
              throw std::runtime_error("B_lr must be a CUDA tensor on the same device as the tile");
            }
            // Dtype
            if (B_lr.scalar_type() != c10::CppTypeToScalarType<T>::value) {
              throw std::runtime_error("B_lr dtype must match tile dtype");
            }
            
            // Ensure column-major [rank, x], strides [1, rank]
            const bool is_col_major = (B_lr.stride(0) == 1 && B_lr.stride(1) == rank);
            auto opts = B_lr.options().device(torch::kCUDA, self.getGPUId()).requires_grad(false);
            torch::Tensor cm = is_col_major ? B_lr
                                            : torch::empty_strided({rank, x}, {1, rank}, opts);
            if (!is_col_major) {
              cm.copy_(B_lr);
            }
            
            self.finishUpdateCalculations();
            std::lock_guard<std::mutex> lock(self.mutex_);
            self.setExternalStream(at::cuda::getCurrentCUDAStream());
            lrtt_dev->copyBLRFrom(
                reinterpret_cast<const T_RPU*>(cm.template data_ptr<T>()),
                self.getStream());
            self.finishAllCalculations();
            self.releaseExternalStream();
          },
          py::arg("B_lr"),
          R"pbdoc(
           Set the B low-rank matrix in LR-TT device.
           
           Args:
               B_lr: [rank, x_size] CUDA tensor on the same device as the tile.
                     Preferred layout: column-major (strides [1, rank]).
                     Row-major inputs are accepted and internally converted.
           )pbdoc");
};
#undef NAME

template void declare_rpu_tiles_cuda<float, float>(py::module &, std::string, bool);
#ifdef RPU_USE_DOUBLE
template void declare_rpu_tiles_cuda<double, double>(py::module &, std::string, bool);
#endif
#ifdef RPU_USE_FP16
template void declare_rpu_tiles_cuda<at::Half, half_t>(py::module &, std::string, bool);
#endif

#endif
